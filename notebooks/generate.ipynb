{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version\n",
      "------------------ -----------\n",
      "appnope            0.1.4\n",
      "asttokens          2.4.1\n",
      "black              24.4.2\n",
      "certifi            2024.2.2\n",
      "charset-normalizer 3.3.2\n",
      "click              8.1.7\n",
      "comm               0.2.2\n",
      "debugpy            1.8.1\n",
      "decorator          5.1.1\n",
      "executing          2.0.1\n",
      "filelock           3.14.0\n",
      "fsspec             2024.5.0\n",
      "huggingface-hub    0.23.1\n",
      "idna               3.7\n",
      "ipykernel          6.29.4\n",
      "ipython            8.24.0\n",
      "jedi               0.19.1\n",
      "Jinja2             3.1.4\n",
      "jupyter_client     8.6.2\n",
      "jupyter_core       5.7.2\n",
      "MarkupSafe         2.1.5\n",
      "matplotlib-inline  0.1.7\n",
      "mlx                0.14.0\n",
      "mlx-lm             0.14.0\n",
      "mypy-extensions    1.0.0\n",
      "nest-asyncio       1.6.0\n",
      "numpy              1.26.4\n",
      "packaging          24.0\n",
      "parso              0.8.4\n",
      "pathspec           0.12.1\n",
      "pexpect            4.9.0\n",
      "pip                24.0\n",
      "platformdirs       4.2.2\n",
      "prompt-toolkit     3.0.43\n",
      "protobuf           5.27.0\n",
      "psutil             5.9.8\n",
      "ptyprocess         0.7.0\n",
      "pure-eval          0.2.2\n",
      "Pygments           2.18.0\n",
      "python-dateutil    2.9.0.post0\n",
      "PyYAML             6.0.1\n",
      "pyzmq              26.0.3\n",
      "regex              2024.5.15\n",
      "requests           2.32.2\n",
      "safetensors        0.4.3\n",
      "sentencepiece      0.2.0\n",
      "six                1.16.0\n",
      "stack-data         0.6.3\n",
      "tokenizers         0.19.1\n",
      "tornado            6.4\n",
      "tqdm               4.66.4\n",
      "traitlets          5.14.3\n",
      "transformers       4.41.1\n",
      "typing_extensions  4.12.0\n",
      "urllib3            2.2.1\n",
      "wcwidth            0.2.13\n"
     ]
    }
   ],
   "source": [
    "!pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO ID                                     REPO TYPE SIZE ON DISK NB FILES LAST_ACCESSED  LAST_MODIFIED  REFS LOCAL PATH                                                                                      \n",
      "------------------------------------------- --------- ------------ -------- -------------- -------------- ---- ----------------------------------------------------------------------------------------------- \n",
      "dfurman/Mistral-7B-Instruct-v0.3-mlx-4bit   model             4.1G        7 23 hours ago   23 hours ago   main /Users/danielfurman/.cache/huggingface/hub/models--dfurman--Mistral-7B-Instruct-v0.3-mlx-4bit   \n",
      "mlx-community/Meta-Llama-3-8B-Instruct-4bit model             5.3G        6 24 hours ago   24 hours ago   main /Users/danielfurman/.cache/huggingface/hub/models--mlx-community--Meta-Llama-3-8B-Instruct-4bit \n",
      "mlx-community/Phi-3-mini-128k-instruct-4bit model             2.2G        9 2 minutes ago  2 minutes ago  main /Users/danielfurman/.cache/huggingface/hub/models--mlx-community--Phi-3-mini-128k-instruct-4bit \n",
      "mlx-community/aya-23-8B-4bit                model             4.5G        6 3 minutes ago  5 minutes ago  main /Users/danielfurman/.cache/huggingface/hub/models--mlx-community--aya-23-8B-4bit                \n",
      "mlx-community/openchat-3.6-8b-20240522-4bit model             4.5G        6 41 minutes ago 41 minutes ago main /Users/danielfurman/.cache/huggingface/hub/models--mlx-community--openchat-3.6-8b-20240522-4bit \n",
      "\n",
      "Done in 0.0s. Scanned 5 repo(s) for a total of \u001b[1m\u001b[31m20.6G\u001b[0m.\n",
      "\u001b[90mGot 3 warning(s) while scanning. Use -vvv to print details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli scan-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "usage: generate.py [-h] [--model MODEL] [--adapter-path ADAPTER_PATH]\n",
      "                   [--trust-remote-code] [--eos-token EOS_TOKEN]\n",
      "                   [--prompt PROMPT] [--max-tokens MAX_TOKENS] [--temp TEMP]\n",
      "                   [--top-p TOP_P] [--seed SEED] [--ignore-chat-template]\n",
      "                   [--use-default-chat-template] [--colorize]\n",
      "                   [--cache-limit-gb CACHE_LIMIT_GB]\n",
      "\n",
      "LLM inference script\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model MODEL         The path to the local model directory or Hugging Face\n",
      "                        repo.\n",
      "  --adapter-path ADAPTER_PATH\n",
      "                        Optional path for the trained adapter weights and\n",
      "                        config.\n",
      "  --trust-remote-code   Enable trusting remote code for tokenizer\n",
      "  --eos-token EOS_TOKEN\n",
      "                        End of sequence token for tokenizer\n",
      "  --prompt PROMPT       Message to be processed by the model\n",
      "  --max-tokens MAX_TOKENS, -m MAX_TOKENS\n",
      "                        Maximum number of tokens to generate\n",
      "  --temp TEMP           Sampling temperature\n",
      "  --top-p TOP_P         Sampling top-p\n",
      "  --seed SEED           PRNG seed\n",
      "  --ignore-chat-template\n",
      "                        Use the raw prompt without the tokenizer's chat\n",
      "                        template.\n",
      "  --use-default-chat-template\n",
      "                        Use the default chat template\n",
      "  --colorize            Colorize output based on T[0] probability\n",
      "  --cache-limit-gb CACHE_LIMIT_GB\n",
      "                        Set the MLX cache limit in GB\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mlx_lm.generate --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# mlx-community/openchat-3.6-8b-20240522-4bit\n",
    "# dfurman/Mistral-7B-Instruct-v0.3-mlx-4bit\n",
    "# mlx-community/Meta-Llama-3-8B-Instruct-4bit\n",
    "# mlx-community/aya-23-8B-4bit\n",
    "# mlx-community/Phi-3-mini-128k-instruct-4bit\n",
    "\n",
    "DEFAULT_MODEL_PATH = \"mlx-community/aya-23-8B-4bit\"\n",
    "DEFAULT_PROMPT = \"Je m'appelle Daniel. Ca va?\"\n",
    "DEFAULT_MAX_TOKENS = 200\n",
    "DEFAULT_TEMP = 0.6\n",
    "DEFAULT_TOP_P = 1.0\n",
    "DEFAULT_SEED = 0\n",
    "\n",
    "SYSTEM_COMMAND = (\n",
    "    f\"\"\"python3 -m mlx_lm.generate \\\n",
    "                --model {DEFAULT_MODEL_PATH} \\\n",
    "                --prompt \"{DEFAULT_PROMPT}\" \\\n",
    "                --max-tokens {DEFAULT_MAX_TOKENS} \\\n",
    "                --temp {DEFAULT_TEMP} \\\n",
    "                --top-p {DEFAULT_TOP_P} \\\n",
    "                --seed {DEFAULT_SEED}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 33509.75it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "response = os.popen(SYSTEM_COMMAND).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Je m'appelle Daniel. Ca va?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "Bonjour Daniel! Je m'appelle Coral, un chatbot d'assistance intelligent. Je suis ravi de faire votre connaissance! Comment puis-je vous aider aujourd'hui?\n",
      "==========\n",
      "Prompt: 25.738 tokens-per-sec\n",
      "Generation: 26.257 tokens-per-sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
