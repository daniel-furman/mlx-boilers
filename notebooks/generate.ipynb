{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.179869184\n",
      "6.719340544\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "print(psutil.virtual_memory().total / 1e9)\n",
    "print(psutil.virtual_memory().available / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!huggingface-cli scan-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "usage: generate.py [-h] [--model MODEL] [--adapter-path ADAPTER_PATH]\n",
      "                   [--trust-remote-code] [--eos-token EOS_TOKEN]\n",
      "                   [--prompt PROMPT] [--max-tokens MAX_TOKENS] [--temp TEMP]\n",
      "                   [--top-p TOP_P] [--seed SEED] [--ignore-chat-template]\n",
      "                   [--use-default-chat-template] [--colorize]\n",
      "                   [--cache-limit-gb CACHE_LIMIT_GB]\n",
      "\n",
      "LLM inference script\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model MODEL         The path to the local model directory or Hugging Face\n",
      "                        repo.\n",
      "  --adapter-path ADAPTER_PATH\n",
      "                        Optional path for the trained adapter weights and\n",
      "                        config.\n",
      "  --trust-remote-code   Enable trusting remote code for tokenizer\n",
      "  --eos-token EOS_TOKEN\n",
      "                        End of sequence token for tokenizer\n",
      "  --prompt PROMPT       Message to be processed by the model\n",
      "  --max-tokens MAX_TOKENS, -m MAX_TOKENS\n",
      "                        Maximum number of tokens to generate\n",
      "  --temp TEMP           Sampling temperature\n",
      "  --top-p TOP_P         Sampling top-p\n",
      "  --seed SEED           PRNG seed\n",
      "  --ignore-chat-template\n",
      "                        Use the raw prompt without the tokenizer's chat\n",
      "                        template.\n",
      "  --use-default-chat-template\n",
      "                        Use the default chat template\n",
      "  --colorize            Colorize output based on T[0] probability\n",
      "  --cache-limit-gb CACHE_LIMIT_GB\n",
      "                        Set the MLX cache limit in GB\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mlx_lm.generate --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# mlx-community/openchat-3.6-8b-20240522-4bit\n",
    "# dfurman/Mistral-7B-Instruct-v0.3-mlx-4bit\n",
    "# mlx-community/Meta-Llama-3-8B-Instruct-4bit\n",
    "# mlx-community/aya-23-8B-4bit\n",
    "# mlx-community/Phi-3-mini-128k-instruct-4bit\n",
    "\n",
    "DEFAULT_MODEL_PATH = \"mlx-community/Meta-Llama-3-8B-Instruct-4bit\"\n",
    "DEFAULT_PROMPT = \"Tell me a recipe for a spicy margarita.\"\n",
    "DEFAULT_MAX_TOKENS = 200\n",
    "DEFAULT_TEMP = 0.6\n",
    "DEFAULT_TOP_P = 1.0\n",
    "DEFAULT_SEED = 0\n",
    "\n",
    "SYSTEM_COMMAND = f\"\"\"python3 -m mlx_lm.generate \\\n",
    "                --model {DEFAULT_MODEL_PATH} \\\n",
    "                --prompt \"{DEFAULT_PROMPT}\" \\\n",
    "                --max-tokens {DEFAULT_MAX_TOKENS} \\\n",
    "                --temp {DEFAULT_TEMP} \\\n",
    "                --top-p {DEFAULT_TOP_P} \\\n",
    "                --seed {DEFAULT_SEED}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python3 -m mlx_lm.generate                 --model mlx-community/Meta-Llama-3-8B-Instruct-4bit                 --prompt \"Tell me a recipe for a spicy margarita.\"                 --max-tokens 200                 --temp 0.6                 --top-p 1.0                 --seed 0\\n    '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SYSTEM_COMMAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 109894.43it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a recipe for a spicy margarita.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "A spicy margarita sounds like a delicious and refreshing cocktail! Here's a recipe that combines the classic margarita flavors with a spicy kick:\n",
      "\n",
      "**Spicy Margarita Recipe**\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 2 ounces tequila (preferably a silver or blanco tequila)\n",
      "* 1 ounce freshly squeezed lime juice\n",
      "* 1/2 ounce triple sec or Cointreau\n",
      "* 1/2 ounce agave syrup (or simple syrup)\n",
      "* 1-2 dashes of jalapeño pepper simple syrup (see below for recipe)\n",
      "* Salt for rimming the glass (optional)\n",
      "* Ice\n",
      "* Lime wedges for garnish\n",
      "* Jalapeño slices or pepperoncini for garnish (optional)\n",
      "\n",
      "**Jalapeño Pepper Simple Syrup Recipe**\n",
      "\n",
      "* 1 cup water\n",
      "* 1 cup granulated sugar\n",
      "* 2-3 sliced jalapeño peppers\n",
      "\n",
      "Combine the water and sugar in a saucepan and heat until the sugar dissolves\n",
      "==========\n",
      "Prompt: 54.720 tokens-per-sec\n",
      "Generation: 27.947 tokens-per-sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(SYSTEM_COMMAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 68200.07it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a recipe for a spicy margarita.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "A spicy margarita sounds like a delicious and refreshing cocktail! Here's a recipe that combines the classic margarita flavors with a spicy kick:\n",
      "\n",
      "**Spicy Margarita Recipe**\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 2 ounces tequila (preferably a silver or blanco tequila)\n",
      "* 1 ounce freshly squeezed lime juice\n",
      "* 1/2 ounce triple sec or Cointreau\n",
      "* 1/2 ounce agave syrup (or simple syrup)\n",
      "* 1-2 dashes of jalapeño pepper simple syrup (see below for recipe)\n",
      "* Salt for rimming the glass (optional)\n",
      "* Ice\n",
      "* Lime wedges for garnish\n",
      "* Jalapeño slices or pepperoncini for garnish (optional)\n",
      "\n",
      "**Jalapeño Pepper Simple Syrup Recipe**\n",
      "\n",
      "* 1 cup water\n",
      "* 1 cup granulated sugar\n",
      "* 2-3 sliced jalapeño peppers\n",
      "\n",
      "Combine the water and sugar in a saucepan and heat until the sugar dissolves\n",
      "==========\n",
      "Prompt: 31.995 tokens-per-sec\n",
      "Generation: 28.210 tokens-per-sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = os.popen(SYSTEM_COMMAND).read()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"\"\"\n",
    "python3 -m mlx_lm.generate \\\n",
    "            --model \"mlx-community/Meta-Llama-3-8B-Instruct-4bit\" \\\n",
    "            --prompt \"Tell me a recipe for a spicy margarita.\" \\\n",
    "            --max-tokens 200 \\\n",
    "            --temp 0.6 \\\n",
    "            --top-p 1.0 \\\n",
    "            --seed 0\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
