{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.179869184\n",
      "9.253552128\n",
      "46.1\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print(psutil.virtual_memory().total / 1e9)\n",
    "print(psutil.virtual_memory().available / 1e9)\n",
    "print(psutil.virtual_memory().percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version\n",
      "------------------ -----------\n",
      "appnope            0.1.4\n",
      "asttokens          2.4.1\n",
      "black              24.4.2\n",
      "certifi            2024.2.2\n",
      "charset-normalizer 3.3.2\n",
      "click              8.1.7\n",
      "comm               0.2.2\n",
      "debugpy            1.8.1\n",
      "decorator          5.1.1\n",
      "executing          2.0.1\n",
      "filelock           3.14.0\n",
      "fsspec             2024.5.0\n",
      "huggingface-hub    0.23.1\n",
      "idna               3.7\n",
      "ipykernel          6.29.4\n",
      "ipython            8.24.0\n",
      "jedi               0.19.1\n",
      "Jinja2             3.1.4\n",
      "jupyter_client     8.6.2\n",
      "jupyter_core       5.7.2\n",
      "MarkupSafe         2.1.5\n",
      "matplotlib-inline  0.1.7\n",
      "mlx                0.14.0\n",
      "mlx-lm             0.14.0\n",
      "mypy-extensions    1.0.0\n",
      "nest-asyncio       1.6.0\n",
      "numpy              1.26.4\n",
      "packaging          24.0\n",
      "parso              0.8.4\n",
      "pathspec           0.12.1\n",
      "pexpect            4.9.0\n",
      "pip                24.0\n",
      "platformdirs       4.2.2\n",
      "prompt-toolkit     3.0.43\n",
      "protobuf           5.27.0\n",
      "psutil             5.9.8\n",
      "ptyprocess         0.7.0\n",
      "pure-eval          0.2.2\n",
      "Pygments           2.18.0\n",
      "python-dateutil    2.9.0.post0\n",
      "PyYAML             6.0.1\n",
      "pyzmq              26.0.3\n",
      "regex              2024.5.15\n",
      "requests           2.32.2\n",
      "safetensors        0.4.3\n",
      "sentencepiece      0.2.0\n",
      "six                1.16.0\n",
      "stack-data         0.6.3\n",
      "tokenizers         0.19.1\n",
      "tornado            6.4\n",
      "tqdm               4.66.4\n",
      "traitlets          5.14.3\n",
      "transformers       4.41.1\n",
      "typing_extensions  4.12.0\n",
      "urllib3            2.2.1\n",
      "wcwidth            0.2.13\n"
     ]
    }
   ],
   "source": [
    "!pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO ID                                     REPO TYPE SIZE ON DISK NB FILES LAST_ACCESSED LAST_MODIFIED REFS LOCAL PATH                                                                                      \n",
      "------------------------------------------- --------- ------------ -------- ------------- ------------- ---- ----------------------------------------------------------------------------------------------- \n",
      "dfurman/Mistral-7B-Instruct-v0.3-mlx-4bit   model             4.1G        7 2 days ago    2 days ago    main /Users/danielfurman/.cache/huggingface/hub/models--dfurman--Mistral-7B-Instruct-v0.3-mlx-4bit   \n",
      "mlx-community/Meta-Llama-3-8B-Instruct-4bit model             5.3G        6 2 days ago    2 days ago    main /Users/danielfurman/.cache/huggingface/hub/models--mlx-community--Meta-Llama-3-8B-Instruct-4bit \n",
      "mlx-community/Phi-3-mini-128k-instruct-4bit model             2.2G        9 16 hours ago  16 hours ago  main /Users/danielfurman/.cache/huggingface/hub/models--mlx-community--Phi-3-mini-128k-instruct-4bit \n",
      "mlx-community/aya-23-8B-4bit                model             4.5G        6 16 hours ago  16 hours ago  main /Users/danielfurman/.cache/huggingface/hub/models--mlx-community--aya-23-8B-4bit                \n",
      "mlx-community/openchat-3.6-8b-20240522-4bit model             4.5G        6 17 hours ago  17 hours ago  main /Users/danielfurman/.cache/huggingface/hub/models--mlx-community--openchat-3.6-8b-20240522-4bit \n",
      "\n",
      "Done in 0.0s. Scanned 5 repo(s) for a total of \u001b[1m\u001b[31m20.6G\u001b[0m.\n",
      "\u001b[90mGot 3 warning(s) while scanning. Use -vvv to print details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli scan-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "usage: generate.py [-h] [--model MODEL] [--adapter-path ADAPTER_PATH]\n",
      "                   [--trust-remote-code] [--eos-token EOS_TOKEN]\n",
      "                   [--prompt PROMPT] [--max-tokens MAX_TOKENS] [--temp TEMP]\n",
      "                   [--top-p TOP_P] [--seed SEED] [--ignore-chat-template]\n",
      "                   [--use-default-chat-template] [--colorize]\n",
      "                   [--cache-limit-gb CACHE_LIMIT_GB]\n",
      "\n",
      "LLM inference script\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model MODEL         The path to the local model directory or Hugging Face\n",
      "                        repo.\n",
      "  --adapter-path ADAPTER_PATH\n",
      "                        Optional path for the trained adapter weights and\n",
      "                        config.\n",
      "  --trust-remote-code   Enable trusting remote code for tokenizer\n",
      "  --eos-token EOS_TOKEN\n",
      "                        End of sequence token for tokenizer\n",
      "  --prompt PROMPT       Message to be processed by the model\n",
      "  --max-tokens MAX_TOKENS, -m MAX_TOKENS\n",
      "                        Maximum number of tokens to generate\n",
      "  --temp TEMP           Sampling temperature\n",
      "  --top-p TOP_P         Sampling top-p\n",
      "  --seed SEED           PRNG seed\n",
      "  --ignore-chat-template\n",
      "                        Use the raw prompt without the tokenizer's chat\n",
      "                        template.\n",
      "  --use-default-chat-template\n",
      "                        Use the default chat template\n",
      "  --colorize            Colorize output based on T[0] probability\n",
      "  --cache-limit-gb CACHE_LIMIT_GB\n",
      "                        Set the MLX cache limit in GB\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mlx_lm.generate --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"\"\"\n",
    "python3 -m mlx_lm.generate \\\n",
    "            --model \"mlx-community/Meta-Llama-3-8B-Instruct-4bit\" \\\n",
    "            --prompt \"Tell me a recipe for a spicy margarita.\" \\\n",
    "            --max-tokens 200 \\\n",
    "            --temp 0.6 \\\n",
    "            --top-p 1.0 \\\n",
    "            --seed 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# mlx-community/openchat-3.6-8b-20240522-4bit\n",
    "# dfurman/Mistral-7B-Instruct-v0.3-mlx-4bit\n",
    "# mlx-community/Meta-Llama-3-8B-Instruct-4bit\n",
    "# mlx-community/aya-23-8B-4bit\n",
    "# mlx-community/Phi-3-mini-128k-instruct-4bit\n",
    "\n",
    "DEFAULT_MODEL_PATH = \"mlx-community/Meta-Llama-3-8B-Instruct-4bit\"\n",
    "DEFAULT_PROMPT = \"Tell me a recipe for a spicy margarita.\"\n",
    "DEFAULT_MAX_TOKENS = 200\n",
    "DEFAULT_TEMP = 0.6\n",
    "DEFAULT_TOP_P = 1.0\n",
    "DEFAULT_SEED = 0\n",
    "\n",
    "SYSTEM_COMMAND = (\n",
    "    f\"\"\"python3 -m mlx_lm.generate \\\n",
    "                --model {DEFAULT_MODEL_PATH} \\\n",
    "                --prompt \"{DEFAULT_PROMPT}\" \\\n",
    "                --max-tokens {DEFAULT_MAX_TOKENS} \\\n",
    "                --temp {DEFAULT_TEMP} \\\n",
    "                --top-p {DEFAULT_TOP_P} \\\n",
    "                --seed {DEFAULT_SEED}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python3 -m mlx_lm.generate                 --model mlx-community/Meta-Llama-3-8B-Instruct-4bit                 --prompt \"Tell me a recipe for a spicy margarita.\"                 --max-tokens 200                 --temp 0.6                 --top-p 1.0                 --seed 0\\n    '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SYSTEM_COMMAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 109894.43it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a recipe for a spicy margarita.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "A spicy margarita sounds like a delicious and refreshing cocktail! Here's a recipe that combines the classic margarita flavors with a spicy kick:\n",
      "\n",
      "**Spicy Margarita Recipe**\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 2 ounces tequila (preferably a silver or blanco tequila)\n",
      "* 1 ounce freshly squeezed lime juice\n",
      "* 1/2 ounce triple sec or Cointreau\n",
      "* 1/2 ounce agave syrup (or simple syrup)\n",
      "* 1-2 dashes of jalapeño pepper simple syrup (see below for recipe)\n",
      "* Salt for rimming the glass (optional)\n",
      "* Ice\n",
      "* Lime wedges for garnish\n",
      "* Jalapeño slices or pepperoncini for garnish (optional)\n",
      "\n",
      "**Jalapeño Pepper Simple Syrup Recipe**\n",
      "\n",
      "* 1 cup water\n",
      "* 1 cup granulated sugar\n",
      "* 2-3 sliced jalapeño peppers\n",
      "\n",
      "Combine the water and sugar in a saucepan and heat until the sugar dissolves\n",
      "==========\n",
      "Prompt: 54.720 tokens-per-sec\n",
      "Generation: 27.947 tokens-per-sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(SYSTEM_COMMAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 68200.07it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a recipe for a spicy margarita.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "A spicy margarita sounds like a delicious and refreshing cocktail! Here's a recipe that combines the classic margarita flavors with a spicy kick:\n",
      "\n",
      "**Spicy Margarita Recipe**\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 2 ounces tequila (preferably a silver or blanco tequila)\n",
      "* 1 ounce freshly squeezed lime juice\n",
      "* 1/2 ounce triple sec or Cointreau\n",
      "* 1/2 ounce agave syrup (or simple syrup)\n",
      "* 1-2 dashes of jalapeño pepper simple syrup (see below for recipe)\n",
      "* Salt for rimming the glass (optional)\n",
      "* Ice\n",
      "* Lime wedges for garnish\n",
      "* Jalapeño slices or pepperoncini for garnish (optional)\n",
      "\n",
      "**Jalapeño Pepper Simple Syrup Recipe**\n",
      "\n",
      "* 1 cup water\n",
      "* 1 cup granulated sugar\n",
      "* 2-3 sliced jalapeño peppers\n",
      "\n",
      "Combine the water and sugar in a saucepan and heat until the sugar dissolves\n",
      "==========\n",
      "Prompt: 31.995 tokens-per-sec\n",
      "Generation: 28.210 tokens-per-sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = os.popen(SYSTEM_COMMAND).read()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
